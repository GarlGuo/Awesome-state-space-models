1. Inverse Approximation Theory for Nonlinear Recurrent Neural Networks (https://openreview.net/forum?id=yC2waD70Vj)

    In particular, we show that nonlinear sequence relationships that can be stably approximated by nonlinear RNNs must have an exponential decaying memory structure - a notion that can be made precise. This extends the previously identified curse of memory in linear RNNs into the general nonlinear setting, and quantifies the essential limitations of the RNN architecture for learning sequential relationships with long-term memory. 
    


2. Sufficient conditions for offline reactivation in recurrent neural networks(https://openreview.net/forum?id=RVrINT6MT7)

    In this study, we develop a mathematical framework that outlines sufficient conditions for the emergence of neural reactivation in circuits that encode features of smoothly varying stimuli. 
    We demonstrate **mathematically** that noisy recurrent networks optimized to track environmental state variables using change-based sensory information naturally develop denoising dynamics, which, in the absence of input, cause the network to revisit state configurations observed during periods of online activity. 
    We validate our findings using numerical experiments on two canonical neuroscience tasks: spatial position estimation based on self-motion cues, and head direction estimation based on angular velocity cues. 

    

3. Composing Recurrent Spiking Neural Networks using Locally-Recurrent Motifs and Risk-Mitigating Architectural Optimization (https://openreview.net/forum?id=uxYye6i2Xi)



4. Leveraging Low-Rank and Sparse Recurrent Connectivity for Robust Closed-Loop Control (https://openreview.net/forum?id=EriR6Ec69a)



5. Exploring the Promise and Limits of Real-Time Recurrent Learning (https://openreview.net/forum?id=V2cBKtdC3a)



6. Conformal Normalization in Recurrent Neural Network of Grid Cells (https://openreview.net/forum?id=wvvj8EPydm)



7. Learning Sequence Attractors in Recurrent Networks with Hidden Neurons (https://openreview.net/forum?id=biNhA3jbHc)



8. Emergent mechanisms for long timescales depend on training curriculum and affect performance in memory tasks (https://openreview.net/forum?id=xwKt6bUkXj)



9. Sparse Spiking Neural Network: Exploiting Heterogeneity in Timescales for Pruning Recurrent SNN (https://openreview.net/forum?id=0jsfesDZDq)



10. Parallelizing non-linear sequential models over the sequence length (https://openreview.net/forum?id=E34AlVLN0v)


