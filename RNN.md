1. Sufficient conditions for offline reactivation in recurrent neural networks(https://openreview.net/forum?id=RVrINT6MT7)

    In this study, we develop a mathematical framework that outlines sufficient conditions for the emergence of neural reactivation in circuits that encode features of smoothly varying stimuli. 
    We demonstrate **mathematically** that noisy recurrent networks optimized to track environmental state variables using change-based sensory information naturally develop denoising dynamics, which, in the absence of input, cause the network to revisit state configurations observed during periods of online activity. 
    We validate our findings using numerical experiments on two canonical neuroscience tasks: spatial position estimation based on self-motion cues, and head direction estimation based on angular velocity cues. 

    

2. Composing Recurrent Spiking Neural Networks using Locally-Recurrent Motifs and Risk-Mitigating Architectural Optimization (https://openreview.net/forum?id=uxYye6i2Xi)



3. Leveraging Low-Rank and Sparse Recurrent Connectivity for Robust Closed-Loop Control (https://openreview.net/forum?id=EriR6Ec69a)



4. Exploring the Promise and Limits of Real-Time Recurrent Learning (https://openreview.net/forum?id=V2cBKtdC3a)



5. Conformal Normalization in Recurrent Neural Network of Grid Cells (https://openreview.net/forum?id=wvvj8EPydm)



6. Learning Sequence Attractors in Recurrent Networks with Hidden Neurons (https://openreview.net/forum?id=biNhA3jbHc)



7. Emergent mechanisms for long timescales depend on training curriculum and affect performance in memory tasks (https://openreview.net/forum?id=xwKt6bUkXj)



8. Sparse Spiking Neural Network: Exploiting Heterogeneity in Timescales for Pruning Recurrent SNN (https://openreview.net/forum?id=0jsfesDZDq)



9. Parallelizing non-linear sequential models over the sequence length (https://openreview.net/forum?id=E34AlVLN0v)

