For sequence modelling, the phenomenon of in-context learning is interesting in the sense that foundation models can be applied to help the downstream tasks. 

1. The mechanistic basis of data dependence and abrupt learning in an in-context classification task (https://openreview.net/forum?id=aN4Jf6Cx69)



2. ONE STEP OF GRADIENT DESCENT IS PROVABLY THE OPTIMAL IN-CONTEXT LEARNER WITH ONE LAYER OF LINEAR SELF-ATTENTION(https://openreview.net/forum?id=8p3fu56lKc&noteId=n3z3Y6l28Q)



3. In-Context Learning Dynamics with Random Binary Sequences (https://openreview.net/forum?id=62K7mALO2q)

    We propose a Cognitive Interpretability framework that enables us to analyze in-context learning dynamics to understand latent concepts in LLMs underlying behavioral patterns. This provides a more nuanced understanding than success-or-failure evaluation benchmarks, but does not require observing internal activations as a mechanistic interpretation of circuits would require.