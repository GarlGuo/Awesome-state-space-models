1. iTransformer: Inverted Transformers Are Effective for Time Series Forecasting: (https://openreview.net/forum?id=JePfAI8fah)

    [GitHub](https://github.com/thuml/iTransformer)

2. CLEX: Continuous Length Extrapolation for Large Language Models (https://openreview.net/forum?id=wXpSidPpc5)

3. Embed-Search-Align: DNA Sequence Alignment using Transformer models (https://openreview.net/forum?id=oNlPtI7QfQ)

4. Linear Log-Normal Attention with Unbiased Concentration (https://openreview.net/forum?id=5nM2AHzqUj)

5. Complexity of Formal Explainability for Sequential Models (https://openreview.net/forum?id=73lu1yw6At)

6. LightSeq: Sequence Level Parallelism for Distributed Training of Long Context Transformers (https://openreview.net/forum?id=kC5i5X9xrn)

7. The Hedgehog & the Porcupine: Expressive Linear Attentions with Softmax Mimicry (https://openreview.net/forum?id=4g02l2N2Nx)

8. The Power of Minimalism in Long Sequence Time-series Forecasting (https://openreview.net/forum?id=hF8jnnexSB)

9. Linear Attention via Orthogonal Memory (https://openreview.net/forum?id=Rn3qJGOitY)
